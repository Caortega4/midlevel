\documentclass[11pt]{article}
\usepackage{graphicx}
\oddsidemargin 1mm

\textwidth 6.3in 
\topmargin -5mm
\headheight 5mm 
\headsep 8mm
\textheight 8.9in

\renewcommand{\baselinestretch}{1.0}
\parindent 0pt
\parskip  4pt

%%=========================================================
\begin{document}
\noindent
\thispagestyle{empty}
\sloppy

\rule{1mm}{0mm}

\vspace{-17mm}
{\LARGE \bf Prosody Principal Components Analysis (PPCA) }
\medskip


{\LARGE \bf Version 4 (Matlab Version) Workflow Notes}
\vspace{7mm}


{\bf Nigel Ward, University of Texas at El Paso}

{\bf \today }

\medskip
\begin{quote}
Abstract: Applying Principal Component Analysis to prosodic features
has been useful for several tasks.  This report describes the
Matlab-based workflow and code.
\end{quote}

\begin{tabular}{p{5cm}l}
& 1 Background  \\
& 2 Overview \\
& 3 File Types \\
& 4 Visualization and Interpretation \\
& 5 Internals\\ 
& 6 Validation \\
& 7 History \\
& 8 Future Work \\
& 9 Local Notes
\end{tabular}


%%=========================================================
\section{Background} 

Principal Components Analysis applied to a large set of prosodic
features spanning various temporal windows can be useful in various
ways.  This gives dimensions which correspond to interpretable
patterns \cite{prosodic-elements}.  The values of these dimensions
usefully character the instantaneous state of the dialog
\cite{dialog-dimensions}.  Applications of these include language
modeling, information retrieval, filtering, and linguistic inquiry
\cite{pca-lm,prosody-ir,sigdial-codec,dimensions-uh-huh}.  Ongoing
work includes: 1) use of the dimensions for speech synthesis, 2)
examining patterns of learner behavior in terms of the dimensions.  We
also foresee applying these techniques to examining differences among
individuals in their use of the dimensions.

This document is written for three audiences: people wanting to learn
how this works, people wanting to get the code working for themselves,
and people wanting to modify or extend the code.


%==========================================================
\section{Overview}

There are two main use cases.  Figure \ref{diagram} overviews how they
relate.

\begin{figure*}[tp]
 \centerline{ 
 \includegraphics[width=11.4cm, trim = 1.cm 4.9cm 0cm 4.05cm, clip=true]{workflow-overview}
}
\caption{Workflow Overview}
\label{diagram}
\end{figure*}


%-------------------------------------------------------
\subsection{Apply Rotation}    \label{applynormrot}
 
This computes, for each moment of a dialog, the values of the
principal components at that moment.  For most purposes this will be
done using some standard, pre-computed principal components, together
with some standard normalization parameters.  (The results may make
more sense if the file to be processed is from the same set as the
audio used to generate the normalization parameters (Section
\ref{computerotation}), thus avoiding potential problems due to
different domains, speaking styles, or languages.  Recording
conditions may also be an issue, although the features are designed to
be somewhat robust to these.)

Thus the Matlab function {\tt applynormrot.m} creates a {\tt .pc} file
for each track of one or more audio files.  The steps are:
\begin{itemize}   \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item read in an audio file 
\item compute the raw  features
\item normalize them, using some precomputed parameters (means and
  standard deviations)
\item  rotate them, using a precomputed rotation
\item  write the results to {\tt .pc} files.
\end{itemize}

The resulting dimensional representation can then be as input to
machine-learning algorithms for various tasks, or can be interpreted.

%$\clubsuit$ every invocation of findDimensions.m or applynormrot.m,
%should be documented in and automatically-created logfile, including
%timing information, written to the same directory the .pc files are
%in, as additional provenance beyond the on-line header.

%------------------------------------------------------------------
\subsection{Compute Rotation} \label{computerotation}

In order to do the above, there of course needs to be a
normalization-and-rotation available to work with.  {\tt
  findDimensions.m} creates this.  The steps are:
\begin{itemize}   \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item read in an audio file 
\item compute the raw  features
\item compute normalization params, then use them to normalize the features
\item compute the rotation, that is, do PCA to discover the dimensions
\item save the rotation coeffs and the norm params for later use (Section \ref{applynormrot})
\end{itemize}

%------------------------------------------------------------------
\subsection{Overview of the Arguments}   \label{arguments}

In general, there are five things needed to completely specify either
of these processes.  Three of these are arguments:

\begin{description}  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item[tracklist] specifies which tracks to process, each being a
  track from an audio file (Section \ref{tracklist-files})
\item[featurespec file]  specifies the set of features to use (Section \ref{featurespec-files})
\item[output dir] specifies where to write the resulting {\tt .pc}
  files (one per track)
\end{description}

The other two things are locations which are, implicitly, the
location where the matlab process is run. 

\begin{description}  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item[pitch cache] subdirectory where to store (or find) the {\tt
  fxrapt}-output f0 values, as {\tt .mat} files
\item[parameter dir] directory where to store (or read) the params and
  coeffs, and the various human-readable files, notably the logfile,
  correlation coefficients, and factor loadings.

\end{description}

Given these implicit locations, it's probably best to create a new
directory for each project.  If all relevant Matlab work is done in
this directory, the all the parameter files will be written here and
then found again without difficulty.  


%%=========================================================
\section{File Types}       

%--------------------------------
\subsection{Data Files}

First there are the data files, each representing an audio track or
file, at various stages of processing.

\begin{description}   \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}

\item [---.au, .wav] The input.  A stereo audio file.  {\tt .wav}
  files sometimes cause trouble for {\tt fxrapt}, so it's better to
  convert them first to {\tt .au} files.  Traditionally these have
  been in Sun format, specifically 16 bit, linear PCM, 8K sampling.

\item[---f0.mat] a file specifying for an audio track the pitch every
  10 milliseconds.  These files are created because fxrapt is slow, so
  it's worth saving the results to avoid needing to ever recompute
  them.

\item[---.pc] The output: a principal components file.  There is a
  one-line header describing the provenance.  Each subsequent line
  describes the prosody at one timepoint.  These are 10ms apart.  Each
  line contains a whitespace-separated list of, first the timepoint,
  then the values for all the principal components (PCs).  PCs appear
  in order of the variance explained.
\end{description}


%%=========================================================
\subsection{Tracklist Files}       \label{tracklist-files}

This specifies the audio tracks to process.  The first line is the
directory in which the audio files are located.  Subsequent lines
specify the track and the file.  For example the line

\begin{quote}
\verb+l sw02079.au+
\end{quote}

means to process the left track of the specified Switchboard audio
file.  Tracklist files have the extension {\tt .tl}. 


%%=========================================================
\subsection{Feature Specification Files}     \label{featurespec-files}

To encode contextual information we need to use features computed at
various temporal offsets, relative to the point of interest.  A
``featureset specification'' ({\tt .fss}) file specifies which
features to use.  These are sometimes called ``crunch'' files since
originally they described how to crunch together data from individual
feature files into a single composite file suitable for machine
learning or dimensionality reduction.


A current feature set under development is {\tt fulltest/al.fss}, an
``assumption light'' new set of mid-level features, including about
168 features

In a {\tt .fss} file each line specifies a feature, a window size, and
an offset, for example

\begin{quote}{\tt 
    vo   -100   -200 self  \\
    cr   -200    400 inte
}\end{quote}

where the first line means the speaker's average volume over a 100ms
window that starts 200ms before the point of interest, and the second
line the interlocutor's average creakiness over a 200ms window that
starts 400ms after the point of interest.  Since {\tt
  getfeaturespec.m} parses these files by fixed offsets, it's
important for everything to line up exactly, and to use spaces not
tabs.


In these files currently the following codes are recognized:

New two-letter codes: 

\begin{tabular}{ll}
  vo  & intensity/volume \\
  sr  & speaking rate proxy \\
  cr  & creakiness \\
  fp  & flat pitch: degree of flatness \\
  np  & narrow pitch range: degree of narrowness \\
  tp  & typical pitch range \\
  wp  & wide pitch range  \\
  hp  & high pitch: degree of highness \\ 
  lp  & low pitch: degree of highness \\
\end{tabular}

Reserved two-letter codes: 

\begin{tabular}{ll}  
  sf      & speaking fraction \\
  vf      & voicing fraction \\
  p0...p5 & pitch bands, to replace lp and hp  \\
  sl      & slowness, to replace sr \\
  fa      & fastness, ditto\\
\end{tabular}



%%=========================================================
\subsection{Normalization and Rotation Parameter File}

{\tt rotationspec.mat}  contains the information pertaining to a
rotation.  This enables the application of an pre-determined rotation 
to new files.  It contains 

\begin{itemize}  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item the normalization parameters, namely for each feature its mean
  and its standard deviation

\item the PCA coefficients
\end{itemize}

A related file is {\tt loadings.txt}, which is a human-readable
version of the PCA coefficients.


%==================================================================
\section{Support for Examining and Interpreting the Results}

Examination of intermediate and final results is important, both to
check that everything is working properly, and to interpret the
results of the process.

%--------------------------------------------
\subsection{Examining the Features}

To see the values of various low-level and mid-level features as they
vary over an audio file, uncomment the various {\tt plot} commands in
{\tt makeTrackMonster.m}.  One can then listen to the audio file,
using any available player, to see whether the feature values are
indeed high and low where they should be.

%--------------------------------------------
\subsection{Examining the Correlations}

As an indirect check on correctness of the feature computation and
collating, one can examine the correlations among the features.  Every
call to {\tt findDimensions.m} creates two correlation files: {\tt
  pre-norm-corr.txt} and {\tt post-norm-corr.txt}, each showing the
most highly correlated and most anticorrelated features for each othe
feature.  These are output by {\tt output\_correlations.m}.

%--------------------------------------------
\subsection{Examining Statistics about the Dimensions}

To see the variance and cumulative variance explained by the PCA-found
dimensions, load the {\tt rotationspec.mat} file and process it with:

\begin{quote}
\begin{verbatim}
load rotationspec
latent ./ sum(latent) 
cumsum(latent) ./ sum(latent) 
pareto(latent ./ sum(latent))   # produces a cool graph
\end{verbatim}
\end{quote}

More interestingly, for each dimension, we'll want to examine
individual variation and (somewhat later) group variation.  The
between-groups comparison will compare all learner data with all
native data, in terms of the two summary statistics, to find out which
dimensions they differ on.

The summary statistics are:
\begin{itemize}   \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item average value (to detect bias to one side of the dimensions)   
\item standard deviation (to detect failure to use a dimension much)
\item skewness
\item kurtosis
\end{itemize}

This is done by {\tt write\_summary\_stats.m}, whose input is the
rotated matrix.  For each column of the matrix (each feature), we
compute these things.  This is called by {\tt applynormrot.m}.  There
is also fragments of a workflow described in {\tt histo/README.txt}:
in short, this uses {\tt distDist.m}, {\tt bhatd.m}, and {\tt
  binProbs.m} to generate histograms for each dimension, including
superimposed histograms for the two populations, and to compute the
Bhattacharyya distance.


%--------------------------------------------
\subsection{Interpreting the Dimensions}

To understand the dimensions, there are three methods to apply.

%---------------------------------
\subsubsection{Examine the Factor Loadings}

{\tt findDimensions.m} includes a call to {\tt writeloadings.m}, which
writes a large, human-readable file called {\tt
  loadings.txt}, the lines of which give the loadings of each feature on each
dimension, for example:

\begin{verbatim}
dimension1  0.12 sel-vo-50+0
dimension1 -1.08 int-ph-400-200
dimension1  0.01 sel-pr+50+100
...
dimension2  0.67 sel-sr+0+100  
...
\end{verbatim}

These files can then be examined to understand the nature of each
dimension.  It's particularly useful to first look at the volume features (with
{\tt grep}) for the ``self'' speaker, to find out when they're
talking.  A useful next step is to look at the ``interloc'' volume
features.  Next it's useful to use the Unix {\tt sort} and {\tt grep}
commands to find, for example, the features with the highest loadings
and those with the strongest (highest absolute) loadings.  

There is also visualization code in  {\tt ppattern.m}.  **


%---------------------------------
\subsubsection{Listen to Extreme Examples}

To understand a dimension, it helps to listen to locations in data
where each dimension has extreme (the highest and lowest) values.
This is done by going into the {\tt .pc} files and finding the
timepoints with extreme values, using {\tt find-extremes.py} This
takes two arguments, the directory of the {\tt .pc} files to process,
and the number of dimensions to process.  The results are written as
textfiles to the {\tt extremes} subdirectory. 

Generally we want the absolutely most extreme points, across all the
files, but may also sometimes want to find one extreme point per file,
for some diversity.

Once we have these timepoints, it's time to listen.  There are lots of
tools that can do this, but we want one that can easily let you jump
to 5 seconds before this point, then play this region.  Invokability
from the command line is a big plus.  Using second notation (not
minutes and seconds) is also nice.  Dede does these things, but only
the 32-bit linux machines run it. One version is in {\tt
  /home/research/isg/speech/workingDede/dede}.  If dede crashes, copy
{\tt /home/research/isg/speech/workingDede/piau-au-file.PCM } to {\tt
  /tmp} and restart it.

In future, it might be nice to automatically feed timepoints to {\tt
  dede}, to direct it to the right places without requiring the user
to view and re-specifiy timepoints.


%---------------------------------
\subsubsection{Consider Co-occurring words} 

The last source of insight for interpreting the dimensions is to see
find which words co-occur with values high/low on each dimension.  Of
course this is only possible if we have transcribed data,
e.g.~Switchboard.  A
workflow for this needs to be revived.


%%=========================================================
\section{Internals}

%--------------------------------------------
\subsection{Frame-Level Feature Computation}

The frame-level (low-level) features are computed: pitch and energy.

The pitch is done with {\tt lookupOrComputePitch.m}, which is a
wrapper for Mike Brookes's Voicebox function {\tt fxrapt.m}; this
gives values in hertz, or NaNs if there is no detectable pitch.

The low-level energy computation is done using {\tt
  computeLogEnergy.m}.

Other frame-level features may later be added.  For example this might
include features  generated by {\tt Praat} (notably NHR).  

If keystrokes are specified in the {\tt .fss} file, {\tt
  featurizeKeystrokes.m} is called to load that information.

%--------------------------------------------
\subsection{Track-Based Normalizations}

Pitch is converted from hertz to percentiles, to normalize for
individual differences in pitch height and in pitch range.

Energy is rescaled to normalize for individual differences and
recording-condition differences in average speaking volume and in
average noise level.  To do this it finds the typical-silence and
typical-speech values of energy, using find\_ss\_cluster\_means.m and
then normalizes the energy with respect to these values.  This is
done, not over the frame-level features, since those are probably too
short, but as part of the subsequent energy-over-larger-window
computations.

(This is not the simplest way to normalize, but it seems suitable.
The average volume across tracks will vary with the amount of
speaking the person in that track is doing.  Thus we want to ensure
that each person, when he is speaking, is reported has having the same
volume on average.  (This is of course not true, since some people have
quieter voices than others, but we can't really detect that.  Also
that probably doesn't matter, since we're only interested, for most
purposes, in whether a speaker is being quiet now relative to his
typical speaking volume.)  There are also slow variations in gain, as
the speaker holds the handset closer or farther over time; these we
also don't deal with.)


%--------------------------------------------
\subsection{Mid-Level Feature Computation}   \label{other-features}

The mid-level features are as listed in Section
\ref{featurespec-files}.  Each summarizes something about the values
of the frame-level features across some window.  The motivations for
these specific choices of feature are in another document: Mid-Level
Prosodic Features for Systematically Investigating Dialog Prosody (in
preparation).
%\cite{mid-level-features}.

Each value is associated with the time at the center of the window.
Windows are shifted (stepped) every 20ms, because it's unlikely that
prosodic features change faster than that.  Windows are always at
least 50ms long, thus they are overlapped.



%--------------------------------------------
\subsection{Feature Assembly}

The relevant features at any point in time are not just those anchored
at that point, but also contextual features from the past or future,
and from the interlocutor as well as the speaker.  We therefore need
to assemble all these features.  
Essentially this just requires concatenating the various mid-level
features, shifted (offset) appropriately.

The output is a huge monster array with {\it nfeatures} columns and
{\it ntimepoints} rows.

For some purposes these assembled features can be useful, as input to
various machine learning algorithms, without going on to the rotation
step.  To write data for such purposes, one can add a call to {\tt
  write\_pc\_file.m} on the monster array.


%--------------------------------------------
\subsection{Overall Normalization}

Before doing PCA we need to normalize the features to all have zero
mean.  It's also helpful for them to have approximately the same
standard deviations, to avoid features with larger variance
dominating.  (The mid-level features are far from normally
distributed, and after normalization that's still true, but this is
probably only an aesthetic  problem.)

%log-pitch is approximately normal.  pitch height, being
%percentile-based, is flat in distribution, our pitch-height and
%pitch-range features are sparse, or, more correctly,  skewed, with lots
%of values near zero and a long tail.
%Energy is bimodal, with 0 being
%typical silence and 1 being typical speech.  ampvar is probably
%trimodal, with high values in consonant-dense regions, moderate values
%in vowels and slower regions, and small values in silence regions.  


Note that we do {\em not} normalize by file.  Any particular speaker
may have his own typical speaking style, and we don't want to lose
that information\footnote{While it's fine to normalize over an entire
  set of dialogs to bring the overall mean to zero, when Shreyas tried
  normalizing, file-by-file, to have each individual file have zero
  mean, all language-modeling benefit was lost.}  (although some has
already been removed by the time we get here, by the normalization of
the pitch and the energy).  Thus we normalize over the monster array, that
is, over the data in all the files in some large set; the same large
set of data that we'll use for the PCA.


%--------------------------------------------
\subsection{Determining the Rotation (doing the PCA)}   \label{rotating}

The PCA itself is done using Matlab's {\tt princomp} function.  This
is memory-intensive.

%-------------------------------------------------------
\subsection{Rotating}

As noted in Section \ref{applynormrot}, this is done by {\tt
  applynormrot.m}, which applies a previously saved {\tt
  rotationspec.mat}, namely the one found in the current directory.

While this and the previous step could be packaged together, currently
they are separate.  (Packaging them together would be convenient for
those times when the files used to determine the rotation are the same
as those we wish to rotate.  Doing so would in addition speed things
up, by avoiding the need to once again read in all the audio files,
compute the features, and normalize them.  However the two steps are
separate for now because we also sometimes need to determine the
dimensions based on one set of data, then apply it to a new set of
data.)

%%=========================================================
\section{Validation}

Testing for most of the feature computation methods was done using
both synthetic test data and also small audio files.  Details are
given in the comments of each Matlab file. 


%%=========================================================
\section{History}

Version 1.  In our language modeling modeling work, we observed
problems due to the non-independence of our prosodic feature set.
Early in 2011 Olac Fuentes suggested we solve this by applying
principal components analysis.  In Summer of 2011 Justin McManus
prototyped the use of PCA on prosodic features for language modeling,
working with just four raw features.

Version 2. Starting Fall 2011, Alejandro Vega extended the code to
handle more features, in particular, making it work for features at
different offsets and over different window sizes, and documented it
in ``Principal Component Analysis on Long Range Prosodic Features'',
available locally at {\tt
  /home/research/isg/speech/uteplm/documentation/howto.tex} and {\tt
  /home/research/isg/speech/timelm/switchboardPCx/documentation/}.  He
applied these to Switchboard data, probably the files listed in {\tt
  fulltest/alex16.tl}.  (The audio files are on the CDs, but some
other sample Switchboard files are in {\tt
  /isg/speech/uteplm/switchboardau/ }.)  The factors loadings this
gave are in {\tt isg/speech/timelm/switchboardPCx/factorLoadings},
generated by {\tt switchboardPCx/factorLoadings.py}.  Extreme examples
for each dimension were found using the {\tt switchboardPCx} version
of {\tt find-extremes.py}.  Some timestamps of extreme points are in
{\tt isg/speech/timelm/switchboardPCx/audioExamples}, and audio clips
for those are in {\tt /home/users/nigel/papers/dimensions/snippets}.
Words correlating with high/low dimension values are in {\tt
  switchboardPCx/countFiles/sratios}.

Version 3. Starting late 2012, I reimplemented almost everything, in
particular, I separated out the PCA code from the language-modeling
code, introduced {\tt .fss} files to made feature assembly
parameterizable, and documented everything.  There are several classic
old standard feature specifications, including
minitest/minicrunch.fss, 11 features for testing the workflow;
social/symmetric.fss, 96 features, used for social speech; and
fulltest/slim.fss, 78 features (48 self and 30 interlocutor), as used
for the narrow-pitch work.  This involved two features which are not
obsolete: ph (pitch height) and pr (pitch range).  This was the
version shared with Columbia, Naver, and Parc.

Version 4.  In Fall 2014 I began to reimplement everything again, this
time in Matlab.  Paola Gallardo did some of the functions, as noted in
the comments.  The motivations were to avoid a hybrid C-Python-Matlab
workflow, to simplify the codebase, to improve portability, to use
more robust features.  The big downside is that for labeling and
analysis, Matlab doesn't seem to support sound integrated with a
display, labeling and user controls, so for those aspects of the work
we still use Elan and dede.  In this version we've also broken the
link to the aizula code for realtime input and output, using
microphone and speakers.


%==================================================================
\section{Future Work}

It would be nice to use a pitch tracker that also outputs probability
of voicing.

The implementation could be made much more efficient. 
particular, work is repeated across features that share computations
(such as narrow pitch and wide pitch), and across different window
sizes of the same feature, and for same-feature-same-window-size
features across different offsets. 

Other mid-level features could be added, as hinted in Section
\ref{featurespec-files}.  For example, this might include {\tt mrate}
(namely speaking rate, although in our Specom 2012 paper we found it
worse than amplitude variation (ampvar, sometimes also called jitter)
as a speaking-rate proxy).

... Find\_extremes.py should be redone in Matlab.  Because adjacent
timepoints typically have similar values, existing extremes files
contain lots of timepoints which are close together.  It might be nice
to add a pruning stage where we drop all points within 1000ms of a
more extreme point.  Also it might be interesting to be able to find
and peruse locations where each dimension has values that are high and
low {\em relative to other values}.  This is important because the
highest instance of dimension x may also be high on dimension y, and
the effects of y may mask those of x.  [**for now, eyeball some
  high/low locations in the{ .pc} file, to figure out how best to
  handle this.  One way to do this might be: 1. In matlab, find the
  norm (the mean absolute value) for every feature.  2. Divide all
  feature values by their feature's norm.  3. For each feature, define
  its ``salience'' at each point as the ratio of that feature's
  absolute value to the max of the absolute values of all other
  feature.  4.  output the filename, the timepoint, the normalized
  value for the feature of interest, the salience, and the name of the
  strongest rival feature.]

%Futuristic stuff
%
%- software to match up a dimension in one rotation to the most-aligned
%dimension in another rotation (similarity metric is
%\verb+sqrt(sum((a-b).^2))+);


%==================================================================
\section{Local Notes}

Unless otherwise specified, everything is
  locally in linux-side directory {\tt
    /home/research/ isg/speech/ppca/}.

\begin{itemize}
\item   This file is {\tt mlv.tex} in {\tt doc/}.
\item  The source code is in {\tt src4}.
\item  readau.m, readwav.m and fxrapt.m (the pitch tracker) are in {\tt voicebox}
\end{itemize}


Linux machine Lisa has 32 GB, which has been adequate for everything
tried so far.

Matlab r2014a currently runs only on the 64-bit machines, e.g. {\tt
  lisa}, so be sure to login or {\tt ssh} there, or else use r2013a,
in {\tt /opt/local/Matlab/}.

The Mid-Level Features document is in
{\tt /home/users/nigel/paers/learners/features.tex}

%%=========================================================
\bibliographystyle{apalike}
\bibliography{../../../z/bib}

%%=========================================================
\end{document}
