\documentclass[11pt]{article}
\usepackage{graphicx}
\oddsidemargin -1mm

\textwidth 6.3in 
\topmargin -5mm
\headheight 5mm 
\headsep 8mm
\textheight 8.9in

\renewcommand{\baselinestretch}{1.0}
\parindent 0pt
\parskip  4pt

%%=========================================================
\begin{document}
\noindent
\thispagestyle{empty}
\sloppy

\rule{1mm}{0mm}

\vspace{-17mm}
{\LARGE \bf Prosody Principal Components Analysis (PPCA) }
\medskip


{\LARGE \bf Workflow Notes}
\vspace{7mm}


{\bf Nigel Ward}

{\bf \today }

Abstract: Applying Principal Component Analysis to prosodic features
has been useful for many tasks.  This report describes the steps and
software we use to do it.  

\bigskip
Specifically it refers to version xxx, dated yyy, uploaded to the web on zzz.

Unless otherwise specified, everything is locally in linux-side
directory {\tt /home/research/isg/speech/ppca/}, including this file,
as {\tt ppca.tex} in {\tt doc/}.

%%=========================================================
\section{History}

Early in 2011 Olac Fuentes suggested we avoid the problems of
independently conditioning on non-independent prosodic features by
applying principal components analysis.  In Summer of 2011 Justin
McManus prototyped the use of PCA on prosodic features for language
modeling, working with just four raw features.  Starting Fall 2011,
Alejandro Vega extended the code to handle more features, in
particular, making it work for features at different offsets and over
different window sizes, and documented it in ``Principal Component
Analysis on Long Range Prosodic Features'', available locally at {\tt
  /home/research/isg/speech/uteplm/documentation/howto.tex} and {\tt
  /home/research/isg/speech/timelm/switchboardPCx/documentation/}.

Starting late 2012, I reimplemented most everything, in particular, I:
\begin{itemize}  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item separated out the PCA code from the language-modeling code
\item made collation of features parameterizable
\item enabled feature-crunching (and normalization) without rotation to
  produce input for machine-learning algorithms, for Karen and Shreyas
\item documented everything
\end{itemize}

Improvements still pending include:
\begin{itemize}   \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item making the whole workflow run on 64-bit Linux 
\item enabling the easy inclusion of new features (Section
  \ref{other-features})
\item supporting realtime computation on microphone input
\end{itemize}

%%==================================================================
\section{Motivation and Basics}

Please read Ward and Vega 2012 (Sigdial) and/or Ward 2014 (Speech
Prosody) before tackling this document or the code.

Other work illustrating the use of (some version of) this workflow
includes our Interspeech 2012 paper applying it to language modeling,
Alex's Masters thesis (2012) evaluating lots of features, my paper on
{\em uh-huh} at the 2012 Feedback Behaviors Workshop, Steve's papers
on utility for audio search (Interspeech 2013, MediaEval 2013, Speech
Communications submitted), and Karen's paper on predicting importance
(Sigdial 2013).

Ongoing work includes: 1) use of the dimensions for speech synthesis,
2) examining patterns of learner behavior in terms of the dimensions.

These techniques could also be applied to:
3) examining differences a) among individuals in their use of the
dimensions, b) in dimensions across dialog types, and c) in dimensions
across languages, 4) discovering the dimensions of different
languages, and 5) enabling other researchers to do some or all of
these things.

This document is written for three audiences: people wanting to learn
more about how this works, people wanting to get the code working for
themselves, and people wanting to modify or extend the code.

%==========================================================
\section{Use Cases}

There are two main use cases.  

(There's a python script {\tt pfrotate.py} (for ``prosodic feature
rotate'') that in theory does everything, but in practice you'll
probably want to run the two separately (in part because our Matlab
only work on 64-bit linux and respond only on 32-bit linux).)


%-------------------------------------------------------
\subsection{Apply Rotation}    \label{applyrot}
 
This computes, for a dialog, the principal components active at each
moment.  This creates a {\tt .pc} file for each track of one or more .au
files.

For most purposes this will be done using some standard, pre-computed
principal components, together with some standard normalization
parameters.

(The results may make more sense if the file to be processed is from
the same set as the audio used to generate the normalization
parameters (Section \ref{computerotation}).  In particular, things may
get strange if domains, speaking styles, or languages are different.
Recording conditions may also be an issue, although the raw features
output by {\tt respond} are somewhat robust to these.)

The steps involved are:
\begin{itemize}   \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item  compute and read in the features
\item  fancy-normalize them, using some precomputed parameters
\item  rotate them, using a precomputed rotation
\end{itemize}

%------------------------------------------------------------------
\subsection{Compute Rotation} \label{computerotation}

In order to do the above, there of course needs to be a
normalization-and-rotation available to work with. 

The steps are:
\begin{itemize}   \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item compute and read in the features
\item compute normalization params, then use them to fancy-normalize the features
\item compute the rotation, that is, do PCA to discover the dimensions
\item save the rotation coeffs and the norm params for later use (Section \ref{applyrot})
\end{itemize}

%------------------------------------------------------------------
\subsection{Overview of the Arguments} 

In general, there are five things involved in specifying a PCA process:

\begin{description}  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item[tracklist] file specifying which tracks to process, each being a
  track from an audio file.  Specifies the directory, then for each
  track the filename and the channel (left or right)
\item[featurespec file] file specifying the set of features to use (Section \ref{featurespec-files})
\item[feature dir] directory where to store (or find) the temporary
  files (.ep and .if) 
\item[parameter dir] directory where to store (or read) the params and
  coeffs, and the various human-readable files, notably the logfile,
  correlation coefficients, and factor loadings
\item[output dir] directory where to write the resulting {\tt .pc} files (one per track)
\end{description}


%%=========================================================
\section{File Types}       

%--------------------------------
\subsection{Data Files}

First there are the data files, each representing an audio track or
file, at various stages of processing.

\begin{description}   \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}

\item [.au] The input.  An audio file in Sun format, specifically 16
  bit, linear PCM, 8K sampling, stereo.  These are typically created
  by applying sox to convert a .wav file.

\item[.f0] a pitch file with a pitch point every 10 milliseconds 

\item[.ep] an Energy-and-Pitch-feature file

\item[.if] an individual-feature file, described later.

\item[.pc] The output: a principal components file.  There is no
  header.  Each line describes the prosody at one point.  Points are
  10ms apart.  Each line contains a whitespace-separated list of the
  values for all the principal components, in order of variance
  explained.  This has the same format as the {\tt .ep} files. 
\end{description}

Note that all but the initial .au file can be recomputed, so they
don't need to be saved.  In particular, the {\tt .ep} and {\tt .if}
files are intermediate files that have no independent value, so need
not be kept, except possibly to save computation time.


%%=========================================================
\subsection{Tracklist Files}       \label{tracklist-files}

This specifies the audio tracks to process.  The first line is the
directory in which the audio files are located.  (Old files lack this
and it needs to be added.)  Subsequent lines specify the track and the
file.  For example the line

\begin{verbatim}
l sw02079.au
\end{verbatim}

means to process the left track of the specified Switchboard audio
file.  Tracklist files have the extension {\tt .tl}. 

A standard tracklist, which is probably the same used in our Sigdial
2012 paper, is {\tt fulltest/alex16.tl}.

Tracklists are used to directly specify the {\tt .au} files to compute
features for, for {\tt multirespond.py}.  They are also used to
partially specify the feature files to use for {\tt createnormrot.m}
and {\tt .applynormrot.m}; in those cases the directory to use and the
extension to use are different, the former is passed in as
a parameter, and the latter is hardcoded.


%%=========================================================
\subsection{Feature Specification Files}     \label{featurespec-files}

To encode contextual information we need to use features computed at
various temporal offsets, relative to the point of interest.  A
``featureset specification'' ({\tt .fss}) file specifies which
features to use.  These are sometimes called ``crunch'' files since
they describe how to crunch together data from multiple feature files
into a single composite file suitable for machine learning or
dimensionality reduction.  

In general, it's probably fine to use one of the standard feature
specifications, such as:
\begin{description}  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item[minitest/minicrunch.fss], 11 features, for testing the workflow
\item[social/symmetric.fss], 96 features, used for social speech
\item[fulltest/fullcrunch.fss], 112 features, too many to use except on a big-memory machine
\item[fulltest/slim.fss], 78 features, 48 self, 30 interlocutor, the new standard set 
\end{description}

In a {\tt .fss} file each line specifies a feature, a window size, and
an offset, for example

\begin{quote}{\tt 
    pitch-100 -200 self  \\
    volume-200 400 interloc
}\end{quote}

where the first line means the speaker's average pitch over a 100ms
window that starts 200ms before the point of interest, and the second
line the interlocutor's average volume over a 200ms window that starts
400ms after the point of interest.

Since {\tt getfeaturespec.m} parses this by fixed offsets, it's
important for everything to line up exactly, and to use spaces not
tabs.

{\tt .fss} files are used specifically in:

\begin{description}  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item[multirespond.py], to determine what window sizes to invoke {\tt respond} with.

\item[load\_features.m], to determine which features to read in and assemble. 

\item[fetch\_and\_preen.m] called by the above

\item[getfeaturespec.m], which is  the subroutine that actually parses the features, called in many places
\end{description}

%%=========================================================
\subsection{rotationspec.mat}

This is a matlab file that contains the information pertaining to a
rotation.  This enables the application of an pre-determined rotation 
to new files.  It contains 

\begin{itemize}  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item the normalization parameters, namely for each feature its mean
  and its standard deviation

\item the PCA coefficients
\end{itemize}

A related file is {\tt loadings.txt}, which is a human-readable
version of the PCA coefficients.


%%=========================================================
\section{File Organization}

It's probably best to create a new directory for each project.  If all
relevant Matlab work is done in this directory, {\tt rotationspec.mat}
will be written here and then found again without difficulty.  This
directory will also naturally house the various human-readable files
and the logfiles.  The {\tt .pc} output files can also sensibly go here.

However there's no need to store the temporary files in this
directory.  Since currently matlab and respond don't run on the same
machine can't use {\tt /tmp} for this.  A handy place to use for this
is {\tt epPark}; this is hardcoded in {\tt multrespond.py}.


%%=========================================================
\section{Code}

Figure \ref{diagram} overviews the components.

\begin{figure*}[tp]
 \centerline{ 
 \includegraphics[width=12cm, trim = 1.cm 1cm 1cm 1cm, clip=true]{ppca-workflow}
 }
\caption{Workflow Overview}
\label{diagram}
\end{figure*}


%--------------------------------------------
\subsection{Basic Now-Feature Production}

PCA starts with some collection of raw features.  These are generated
in two steps, both conveniently done by the python program
{\tt multirespond.py} (Section \ref{multirespond}).

Multirespond first calls a C program, {\tt respond}, to do the actual
signal processing.  {\tt respond} program takes a {\tt .au}-format
stereo file and writes two {\tt .ep} files, one for each track.  It
does this for four base features: volume, pitch range, pitch height,
and frame-by-frame amplitude variation (``ampvar''), as a proxy for
speaking rate.

The working version of respond is in {\tt
  uteplm/alex\_aizula\_test/out/}.

Internally {\tt respond} first does some preparatory work: computing
the raw energy, and invoking {\tt f0calc} to compute the raw pitch.
Second it normalizes the energy, to overcome any recording-condition
differences in average speaking volume and in average noise level.  To
do this it finds the typical-silence and typical-speech values of
energy, and then normalizes the energy with respect to these
values. Third, it cleans the pitch values and converts them to
percentiles.  This normalizes for individual differences in pitch
height and in pitch range.

Finally it computes the four features.  These can be computed over
windows of various widths, specified as arguments.  Each window is
``anchored'' by its right edge, thus in the output the feature values
for time t are actually the values computed over windows ending at
time t.  If a window extends beyond the start of a file, then the
value is {\tt -1}, thus flagged as invalid.  Similarly, there is a
{\tt -1} flag for windows in which the pitch height cannot be computed
because there are no pitch points, and for windows where the range
cannot be computed because there are less than two pitch points.

%--------------------------------------------
\subsubsection{Other Potential Now Features}   \label{other-features}

Other raw features may later be added.  For example this might include
features are generated by {\tt Praat} (notably NHR) and by {\tt mrate}
(namely speaking rate, although in our Specom 2012 paper we found it
worse than amplitude variation (ampvar, sometimes also called jitter)
as a speaking-rate proxy).

Respond could also be extended to generate a voicing-fraction feature
and maybe a speaking-fraction feature, to say for each window in which
fraction of the frames there was voicing, respectively, speech.

For the future discovery of visualization-friendly dimensions, we'll
want to use filterbank features, for example percent of frames in this
window where the sound fell in the 80--100\%ile energy range, percent
where it fell in the 0--20\%ile pitch region, etc. This would also
obviate the need to  patch in average pitch values to replace non-existing pitch
points.  This would also enable a nice handling of the Barnes effect
(Jon Barnes, Speech Prosody 2014 etc.), that the perceptual
significance of pitch points is proportional to the volume at that
moment.

Note that NHR and the filterbank features only take on non-negative
values, something to consider when designing normalization.


%--------------------------------------------
\subsection{Computing Features over Various Widths}  \label{multirespond}

Each invocation of {\tt respond} only gives results for a single
window size of each type, thus only one pitch-height feature, one
pitch-range feature, etc.  Thus to produce windows of various sizes,
we need to invoke {\tt respond} multiple times.

(Alex had a different way to generate values of windows for multiple
sizes.  He used the code in timelm/switchboardPCx/javaScripts/PCAJava/
to produce mock longer-window estimates by just averaging the values
for adjacent small windows.  While this seemed to work fine, it's not
the obvious way to compute pitch height and pitch range over larger
windows.)

{\tt multirespond.py} does this.  It two arguments, a tracklist file, and a
featurespec file, as described above. 
Thus for example, from {\tt src/}:

\begin{quote}
python 
import multirespond.py
multirespond.everything("../minitest/minitracklist.tl", "../minitest/minicrunch.fss")
\end{quote}

First, from the featurespec file, it infers what the window sizes it
needs to generate.  For example, it might discover that it needs to
produce features over windows of sizes 50, 100, 200, 400, 800, and
1600 (all in milliseconds).

Next it invokes {\tt respond} for each track and each window size.
This produces oodles of {\tt .ep} files, which are all written to
feature directory, traditionally {\tt epPark}.

To make this information easier for matlab to parse (specifically {\tt
  load\_features.m)}, multirespond then ``splits'' these to create
individual-feature files {\tt .if} files (formerly {\tt .sf files}),
each containing has the data for only one feature.  The result is
multiple oodles of files with names like {\tt sw2001-l-ph-100.if} and
{\tt sw2001-r-pr-200.if}.  These also go in the feature directory.  To
save space, these do not contain timestamps; they have just one value
per line.

This step is not quite trivial because {\tt .ep} files lack values
(and even lines) for the first few frames.  This is because, for
example, when {\tt respond} is asked to compute features with window
size 50ms, it outputs no data for the first 5 frames, so the {\tt .ep}
file starts at 50ms.  As part of the splitting process, {\tt
  multirespond} therefore pads the start of each {\tt .if} file with
the appropriate number of lines, using -1 (invalid) as the value
there.  For this it uses {\tt longpadding.txt}. Thus the {\tt .if}
files end up all aligned properly, all starting at time 0.

Note that {\tt importdata} appears to be taking much longer than is
reasonable, and other Matlab users have noted this too, with no
solution suggested, alas.  

%--------------------------------------------
\subsection{Feature Assembly/Collating (``TimeCrunching'')}

The relevant features at any point in time are not just those anchored
at that point, but also contextual features from the past or future,
and from the interlocutor as well as the speaker.  We therefore need
to asemble all these features from the various sources (the various
{\tt .if} files.

(This was previously handled by Alex's {\tt
  timelm/switchboardPCx/pScripts/genXtraFeatures.py}, with the
parameters hard-coded, plus a separate step to fold in the
interlocutor-track features, using {\tt
  timelm/switchboardPCx/pScripts/padinterlocutor.py}.)

This is done in Matlab by {\tt load\_features.m} which assembles
everything specified in the {\tt .fss} file.  The output is a huge
array with {\it nfeatures} columns (some shifted it up or down in time
according to the specification) and {\it ntimepoints} rows.  This
output is used directly by the calling program, either
{\tt createnormrot.m} or {\tt applynormrot.m}.

Local note: Matlab currently runs only on the 64-bit machines,
e.g. {\tt lisa}, so be sure to ssh there.

If one wants to just write the crunched features, there's an option
here to do so, currently commented out.  This writes the features as a
huge {\tt .crf} file (for crunched-raw-feature file), which one might
then use for machine learning.  ``Raw'' here simply means ``not
rotated by PCA''.

%--------------------------------------------
\subsection{Examining the Correlations}

One can examine the correlations among the features.  This can be for
general edification or as an indirect check on correctness of the
feature computation and collating.   In particular we can look for and
examine the most highly correlated few columns, and the least highly
correlated few.  

To make the correlations readable, we map them back to the names of
the features involved.  This is done by {\tt output\_correlations.m},
which is called from {\tt load\_features.m} and writes a
human-readable file, {\tt pre-norm-corr.txt}.  Post-normalization
correlations are also available; these are written by {\tt computenormrot.m}. 


%--------------------------------------------
\subsection{Normalization}

PCA requires that we normalize the features. It's important that they
have zero-mean, and also that they have approximately the same
standard deviations.

Note that we do {\em not} normalize by file.  Any particular speaker
may have his own typical speaking style, and we don't want to kill
that information (although some has already been removed by the time
we get here, by the normalization inside {\tt respond}, as discussed
above).  Thus we normalize over all the files in some large set; the
same large set of data that we'll use for the PCA.

This is done by {\tt createnormalrot.m} where first reads the features
using {\tt load\_features.m} then normalizes them with {\tt
  compute\_normalization\_params} and then {\tt fancy\_normalize.m}.
It then continues on to do PCA (Section \ref{rotating}).

(Alex's normalization code was {\tt
  timelm/switchboardPCx/pScripts/normalizeEP.py}.)


%------------------------------------------------------------------
\subsubsection{Thoughts on Normalization}

Correct normalization is a huge issue.  

Here the purpose of normalization is just to make sure the features
all have the same standard deviations, so that none unnaturally
dominates the PCA.  So z-normalization is fine, and it doesn't matter
that the incoming distributions are largely not normal.  (Pitch
height, being percentile-based, is flat in distribution.  Energy is
bimodal, with 0 being typical silence and 1 being typical speech.
ampvar is probably unimodal, with high values in consonant-dense
regions, moderate values in slower regions, and small values in
silence regions.  The distribution of the pitch range values is
probably skewed, with lots of values near zero and a long tail.)


Now to comment on each feature in turn:

\begin{itemize}   \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item Volume.  Volume varies enormously across tracks, mostly due to
  differences in recording conditions.  Normalizing for this is our
  main concern.  The average volume across tracks will, of course,
  vary with the amount of speaking the person in that track is doing.
  Thus we want to ensure that each person, when he is speaking, is
  reported has having the same average volume.  This is of course not
  true, since some people have quieter voices than others, but we
  can't really detect that.  Also that probably doesn't matter, since
  we're only interested, for most purposes, in whether a speaker is
  being quiet now relative to his typical speaking volume.  There are
  also slow variations in gain, as the speaker holds the handset
  closer or farther over time; these we also don't deal with.  The
  bottom line is, the values output by respond are already
  track-normalized, and should not be messed with.  Of course, we can
  normalize over an entire set of dialogs to bring the overall mean to
  zero, for example, but when Shreyas normalized, file-by-file, to
  have each individual file have zero mean, then all language-modeling
  benefit was lost.
\item Pitch Height.  Pitch height will be essentially robust to the
  recording conditions.  However it is, like volume, also very
  speaker-independent.  By expressing it in percentiles, we have
  already compensated for this.

  Pitch features are tricky, both pitch height and pitch range, since we
  need to deal with the -1 flags.  Here's a solution (not a great one):
  we compute the average value over the valid (not -1) points, then we
  substitute this average values for all the -1s. Then we happily
  z-normalize everything.

\item Pitch Range.  The correct way to normalize this is far from
  clear.  Some pitch patterns, such as the minor third, should be
  normalized in terms of semitones (percent change in Hz relative to
  the starting point).  However for other things, such as jump to a
  high pitch for emphasis, what types of variation are perceptually
  equivalent, is unknown.  For now, this is also represented in terms
  of percentiles.  This is adequate for distinguishing between
  monotone and everything else (including excited speech, swoopy
  speech, and specific pitch features such as sharp pitch drops).
\item Speaking Rate.  This is track-normalized in {\tt respond},
  although the quality of this has only been spot-checked. 
\end{itemize}



%--------------------------------------------
\subsection{Determining the Rotation (doing the PCA)}   \label{rotating}

Done in {\tt createnormrot.m} (create normalization and rotation).

This creates a set of normalization parameters and factor loadings,
and writes it to {\tt rotationspec.mat} for later use by {\tt
  applynormrot.m}.

This first calls the code to load, preen, and normalize the features.

Then comes the PCA.  This is done using Matlab's {\tt princomp}
function.  (This is memory-intensive.  The new Lisa has 32 Gb.  Note
that on a 4GB Alex's machine can only handle 600K datapoints with 76
features, or 450K with 103 features.  Might downsample to every 20ms
to be able to handle effectively more data.

%--------------------------------------------
\subsection{Examining the Dimensions}

After the PCA, we'll often want to examine the variance explained by
the dimensions, and the cumulative variance explained.  This can be
done by loading the {\tt rotationspec.mat} file and then applying:

\begin{verbatim}
load rotationspec
latent ./ sum(latent) 
cumsum(latent) ./ sum(latent) 
pareto(latent ./ sum(latent))   # produces a cool graph
\end{verbatim}


We will also often wish to analyze the dimensions.  
{\tt createnormrot.m} includes a call to  {\tt writeloadings.m}, which
takes as input the {\tt coeff} matrix returned by PCA, and the
featurename information assembled by {\tt getfeaturespec.m}.  Each
column of coef represents the feature loadings for one dimension.  For
each, we output a list of featurename-loading pairs, as one large
file, called {\tt loadings.txt}, for example 

\begin{verbatim}
dimension1  0.12 sel-vo-50+0
dimension1 -1.08 int-ph-400-200
dimension1  0.01 sel-pr+50+100
...
dimension2  0.67 sel-sr+0+100  
...
\end{verbatim}

Such a file can then be explored to find out things about the features
involved in each dimension, for example, using the Unix {\tt sort} and
{\tt grep} commands to find, for example, the features with the
highest loadings, with the strongest (highest absolute) loadings, the
volume features with the highest/lowest loadings (via {\tt grep}), and
so on.

For example, 

\begin{verbatim}
sort -k2n xxx.pc  
awk '{print $2*$2, $0}' xxx.pc | sort -n | cut -d " " -f2-
\end{verbatim}

To truly understand a dimension, we also will want to find datapoints
high/low on that dimension and listen to what's happening in the
dialog there.  This is possibly after applying a rotation to generate
{\tt .pc} files, as described in the next section. 


%-------------------------------------------------------
\subsection{Rotate}

This is done by {\tt applynormrot.m}, which applies a previously saved
{\tt rotationspec.mat}, namely the one found in the current directory.

This and the previous step are packaged up as {\tt twostep.m}.

%--------------------------------------------
\subsection{Examining  Variation in Uses of the Dimensions}

For each dimension, we'll want to examine individual variation 
and (somewhat later) group variation.

The between-groups comparison will compare all learner data with all
native data, in terms of the two summary statistics, to find out which
dimensions they differ on.

The two summary statistics are:
\begin{itemize}   \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item average value (to detect bias to one side of the dimensions)   
\item average absolute value (to detect failure to use a dimension much)
\end{itemize}

In addition, it would be nice to generate histograms for each
dimension, including superimposed histograms for the two populations.

This is done by {\tt write\_summary\_stats.m}, whose input is the
rotated matrix.  For each column of the matrix (each feature), we
compute these things.  This is called by {\tt applynormrot.m}.

%--------------------------------------------
\subsection{Interpreting the Dimensions}

To understand the dimensions, various things can be done.

%---------------------------------
\subsubsection{Examine the Factor Loadings}

Look at the numbers.  A useful thing is first to look at the volume
features (with {\tt grep}) for the ``self'' speaker, to find out when
they're talking.  A useful next step is to look at the ``interloc''
volume features.  Next it's useful to use {\tt sort} to find the 
strongly negative and strongly positive factors on this dimension. 

  a. visualize them, using the text file {\tt loadings.txt} as
  mentioned above.  Also the code in {\tt
    $\sim$/papers/dublin/pattern.m} and {\tt ppattern.m}


Old factor-loading examples are in {\tt
  isg/speech/timelm/switchboardPCx/factorLoadings}, generated by
{\tt switchboardPCx/factorLoadings.py}.


%---------------------------------
\subsubsection{Listen to Extreme Examples}

To help understand a dimension, it's helpful to listen to locations
where each dimension has extreme (the highest and lowest) values.

This is done with {\tt find-extremes.py}, which takes two arguments,
the directory of the {\tt .pc} files to process, and the number of
dimensions to process.  The results are written as textfiles to the
{\tt extremes} subdirectory.

In the past, these extreme points were found using the {\tt
  switchboardPCx} version of {\tt find-extremes.py}.  Some timestamps
of extremes on Alex's analysis are in {\tt
  isg/speech/timelm/switchboardPCx/audioExamples}, and audio clips for
those are in {\tt /home/users/nigel/papers/dimensions/snippets}.

One issue is that adjacent timepoints typically have similar values.
So there ought to be a pruning stage where we drop all points within
500ms of a more extreme point, either in matlab, or in python (perhaps
by a call to Unix's uniq).

Generally we want the absolutely most extreme points, across all the
files, but may also sometimes want to find one extreme point per file,
for some diversity.

Once we have these timepoints, it's time to listen.  There are lots of
tools that can do this, but we want one that can easily let you jump
to 5 seconds before this point, then play this region.  Invokable from
the command line is a big plus.  Using second notation (not minutes
and seconds) is also nice.  Dede used to do all these things, but no
current machine seems to run it.

To Do:
\begin{itemize}   \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}
\item to write dede calls to play these and 
\item sox calls to clip out snippets of these and optionally add a 
  a ``beep'' to mark the precisely  the most extreme point in that clip
\item python calls to lookup the words said at those points
\end{itemize}

Also to do: find locations where each dimension has values that are
high and low {\em relative to other values}.  This is important
because the highest instance of dimension x may also be high on
dimension y, and the effects of y may mask those of x.  **for now,
eyeball some high/low locations in the .pc file, to figure out how
best to handle this.  Some preliminary thoughts:

- In matlab, find the norm (the mean absolute value) for every feature.

- Divide all feature values by their feature's norm.  

- For each feature, define its ``salience'' at each point as the ratio
of that feature's absolute value to the max of the absolute values of
all other feature.

- output the filename, the timepoint, the normalized value for the
feature of interest, the salience, and the name of the strongest rival
feature

\bigskip
Any files can be used for extreme-based dimension interpreting.  If
doing Switchboard, it's nice to work with the files examined for the
Sigdial 2012 paper, for the sake of familiarity.  These are listed in
{\tt fulltest/alexscan.tl}

The Switchboard audio is in {\tt
 /isg/speech/uteplm/switchboardau/ } with a longer set in {\tt timelm/switchboardau/aufiles} .  You can listen
to it using {\tt /home/research/isg/speech/workingDede/dede}.  If dede
crashes, copy {\tt
  /home/research/isg/speech/workingDede/piau-au-file.PCM } to {\tt
  /tmp} and restart it.



%---------------------------------
\subsubsection{Consider Co-occurring words} 


find which words co-occur with values high/low on each dimension. 

The old listings are in {\tt              switchboardPCx/countFiles/sratios}.



%==========================================================
\section*{Timing }

Multirespond on Abe takes about 15 seconds per 5-minute audio file
with 3 window sizes; more if the .f0 files need to be computed.

Creatnormrot on Lisa takes about 60 seconds per 5-minute track, almost
all of it just loading the feeatures.  40 minutes to do a 78-feature
load on about 60 track-minutes.

4 minutes to write a 10-minute .pc file.

%==========================================================
\section*{To Do }

Quick Stuff

- write a logfile, including timing information


Workflow completions

- support for listening

- support for visualization 

- support for discovering word-dimension tendencies


Bugs and Speed-Ups 

- alter multirespond.py to downsample to half-size the .if files
  i.e., make an interval of 20 ms between samples

- add matlab code to read in a .if file and save it as an equivalent .mat file,
  to save time on subsequent reads.  Might implement as a form of memoization/caching.

- edit respond so that it can be run remotely; the key is to change
it so it doesn't try to doesn't grab the audio device

-  time the various steps, especially the PCA, as a function of size,
on various machines (32G new Lisa, for example)

L2 paper stuff

- run on Alex's training data, compare the resulting dimensions
with his (look at feature loadings,  extreme-valued datapoints) [8]

-  try on Spanish or Japanese [40]

Futuristic stuff

-  try with filterbank-style features

-  measure robustness of derived features to variations in training
set; software to match up a dimension in one rotation to the
most-aligned dimension in another rotation (similarity metric is
\verb+sqrt(sum((a-b).^2))+);


%%=========================================================
\end{document}
