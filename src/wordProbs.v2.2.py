# -*- coding: utf-8 -*-
"""
@author: Aaron Alarcon, The Universty of Texas at El Paso
@version: 2.2  output format modified by Nigel Ward, Nov 20, 2019 
@since: October 4, 2019
Written using Spyder 3.3.3 on Anaconda Navigator 1.9.7

Function: identify words characteristic of extreme points on certain
dimensions of local prosodic contexts in dialog, as a way to help
understand the nature of each dimension. Thus this is part of the Midlevel Prosodic Features 
toolkit, as part of the PCA workflow, and should be done after the applynormrot step.
    
Inputs:  This program assumes that it is in the same folder
as the extremes files (dim01.txt, etc, generated by applynormrot in Matlab)
and as the callhome transcripts (xxx.cha, etc, downloaded e.g. from talkbank.org). 
 
Input 1: A set of files, one per dimension.  Each lists, for various
dialog files, the timepoints of two kinds of extremes: the highest ten
places on that dimension and the lowest ten places on that dimension.

Input 2: A set of files, each the transcript of one dialog, as in the
Callhome corpora.  These list the start and end times of utterances,
but unfortunately not of the individual words. Since we don't have per-word 
timestamps, we will count all words in utterances that contain an extreme timepoint.  
Thus we will not know what words are common exactly at these timepoints, 
but only in the vicinity of these timepoints.

Output: For each dimension, two files, one for each "side" of the dimension,
each containing a list of words especially common or uncommon arround extremes
on that side. 

To Run: Install anaconda.  Then from the Start menu invoke "anaconda prompt",
then from there type "spyder" to get the IDE.  From there, open this file and run it.

References: N. Ward and A. Vega, Sigdial 2012; N. Ward, Cambridge 2019

Note: if you wish to use the "traverse_dimensions" function to see 
the number of occurences for each word in every dictionary, you
must call "traverse_dimensions" before calling "analyze" because "analyze" 
as a side effect deletes items from the dictionaries. The results of
"traverse_dimensions" will show up in the same directory your file is saved in,
generating 5 files per dimension included. 


 Note: A and B behaviors are merged within or across sides, as appropriate
for each specific dimension.  This is since each dimension is symmetric in one
 of two ways.  Either the A and B behaviors are the same (dimensions 4, 7, and 10), 
 or they are opposite (all the rest). In either case we can aggregate, to get more robust
counts.  Specifically, the aggregates are done as follows.  (The names are just mnemonics.)

Dim 1 in-turn: sum of the high R and low H 
Dim 1 listening: sum of the high L and low R

Dim 2 yielding: high L and low R
Dim 2 taking: high R and low H

Dim 3 cont: high L and low R
Dim 3 bc: high R and low R

Dim 4 overlap: high L and *high* R 
Dim 4 pause: low L and *low* R 

Dim 5 particle-assisted take: high L and low R
Dim 5 particle-assisted yield: high R and low L

Dim 6 short low: high L and low R
Dim 6 short high: high R and low L

Dim 7 short-louder: high L and *high* R
Dim 7 short-quieter: low L and *low* R

Dim 8 interleaved-two: high L and low R
Dim 8 interleaved-three: low L and high R

Dim 9 high emph: high L and low R
Dim 9 low emph: low L and high R

Dim 10 joint rise: high L and *high* R
Dim 10 joint fall: low L and *low* R


Whether a word is "especially common" is judged by two numbers. The
first the p-value for a chi-square test of the hypothesis that the
word in question is more frequent around the specified extreme points
that it is overall in the corpus, relative to the frequencies of all
other words.  The second is the ratio of the frequency of the word
around the extremes to the frequency of the word in the corpus overall.

For example, if the word "the" occurs 10 times in the vicinity of
"in-turn" extremepoints, and 200 times elsewhere in the corpus, and
there are 400 words total that occur in the vicinity of in-turn
timepoints, and 10000 word total in the corpus, then the ratio is
(10/200) / (200/1000) = .05 / .02 = 2.5. 

Thus the output should look like

Dim 1 in-turn:
    the  p-value: .005  ratio: 2.5
    a    p-value: .020  ratio: 1.8
    if   p-value: .044  ratio: 4.3
    ...
Dim 1 listening:
    uh-huh p-value: .001 ratio: 3.6
    okay   p-value: .003 ratio: 1.1
    hmm    p-value: .014 ratio: 1.8

"""
import io
import re
from scipy.stats import chi2_contingency, fisher_exact
import math
import numpy as np

"""
Holds all information for each dimension of analysis
all the fields except for the filename are dictionaries 
and are named in the format: speaker.characteristic. 
These fields store the information for these four situations
"""
class Dimension(object):
    def __init__(self, filename):
        self.filename = filename
        self.leftLow = {}
        self.leftHigh = {}
        self.rightLow = {}
        self.rightHigh = {}
        self.allWords = {}
        
        

#holds transcript Filename and a list of timestamps containing references to it 
class ThingsToLookFor(object):
    def __init__(self, filename):
        self.filename = filename
        self.timestampInstances = []

    
#holds array of timepoints and an index corresponding to the dimension it's from     
class Timestamps(object):
    def __init__(self, ind, points):
        self.dimInd = ind
        self.points = points
        
"""
read_dims reads every dimension file and stores its information into a list of 
corresponding ThingsToLookFor. It also creates empty dimension objects to be 
operated on later during count_word_occurences and analyze
"""
def read_dims():
    dimensions = list()
    thingsToLookForList = list()
    #Makes an empty reference since we start with dimension 1
    dimensions.append(0)
    #Traverses through dim files
    for dimensionNum in range(1,11):
        #opening dim file and creating thingsToLookFor object to store its data
        print("Starting Dim file ", dimensionNum)
        name = str.format("dim{:02d}.txt", dimensionNum)
        dimfile = open(name, "r")
        
        #make a new dimension object off of our current dimension
        dimensions.append(Dimension(name));
   
        """
        The info line we are reading looks like:
        generated from l of 4104.au, using features.....
        We read it to make sure we have not reached the end of the file
        and to get the filename the analysis is on from it
        """
        info = dimfile.readline().split()
        while(len(info) > 0):
            
            #Getting thingsToLookFor object open
            trimmingOffset = 4
            file = info[4][:trimmingOffset]
            thingsToLookFor = find_thingsToLookFor(thingsToLookForList, file)
               
            #making an empty list of size 4 x 10
            times = np.zeros((4,10)).tolist()
            
            """
            times[0] = left low
            times[1] = left high
            times[2] = right low
            times[3] = right high
            """
            #Loop of 2 across the left and right speaker data found in our file
            for speaker in range(2):
                #Loop of 2 for the 2 sets of data
                #when side is 0, we are on low
                #when side is 1, we are on high
                for side in range(2):
                    #skip unneeded line
                    dimfile.readline()
                    
                    #Traverses through all 10 observances
                    for timepoint in range(10):
                        #Converting timestamp to millis and inserting into its position
                        num = int(float(dimfile.readline().split()[3]) * 1000)
                        if speaker == 0:
                            times[speaker + side][timepoint] = num  
                        else:
                            times[speaker + side + 1][timepoint] = num
                if speaker == 0:
                    #skip unneded line
                    dimfile.readline()
            
            #sorting each subarray in our times for quicker searches later
            for subarray in times:
                subarray.sort()
            
           
            #Appending this reference to our thingsToLookFor object
            thingsToLookFor.timestampInstances.append(Timestamps(dimensionNum, times))
            #getting info for next set of data 
            info = dimfile.readline().split()
        
        #close current dimension file and iterate to next one
        dimfile.close()
    return dimensions, thingsToLookForList
        
        
"""
count_word_occurences goes through each transcript line by line and 
for each time it is referenced by a hash, it checks for the 
timestamps and inserts every line it has into a hash
"""
def count_word_occurences(dimensions, thingsToLookForList):
    #For every thingsToLookforObject
    for thingsToLookFor in thingsToLookForList:
        transcriptText = read_transcript(thingsToLookFor.filename + ".cha");
        for line in range(0, len(transcriptText)-1, 2):
            #setting up bounds and words
            bounds = [int(i) for i in transcriptText[line+1].split("_")]
            words = re.split("\*A:|\*B:| |:|,", transcriptText[line])
            #checking if utterance is left or right
            if "*A" in transcriptText[line][:3]:
                for timestamp in thingsToLookFor.timestampInstances:
                    #seardching for timestamp in whole list
                    timestamp.points[0], resultLL =search(bounds,timestamp.points[0])
                    timestamp.points[1], resultLH = search(bounds,timestamp.points[1])
                   
                    #if found,insert into the respective dictionary
                    for word in words:
                        if len(word) > 0:
                            dimensions[timestamp.dimInd].allWords[word] = dimensions[timestamp.dimInd].allWords.get(word, 0) + 1
                            if resultLL:
                                dimensions[timestamp.dimInd].leftLow[word] = dimensions[timestamp.dimInd].leftLow.get(word, 0) + 1
                            if resultLH:
                                dimensions[timestamp.dimInd].leftHigh[word] = dimensions[timestamp.dimInd].leftHigh.get(word, 0) + 1
                                
            #all right speaker operations
            else:
              for timestamp in thingsToLookFor.timestampInstances:
                  #searching for timestamp in the whole list
                  timestamp.points[2], resultRL = search(bounds,timestamp.points[2])
                  timestamp.points[3], resultRH = search(bounds,timestamp.points[3])
                  
                  #if found, insert into the respective dictionary
                  for word in words:
                      if len(word) > 0:
                          dimensions[timestamp.dimInd].allWords[word] = dimensions[timestamp.dimInd].allWords.get(word, 0) + 1
                          if resultRL:
                              dimensions[timestamp.dimInd].rightLow[word] = dimensions[timestamp.dimInd].rightLow.get(word, 0) + 1
                          if resultRH:
                              dimensions[timestamp.dimInd].rightHigh[word] = dimensions[timestamp.dimInd].rightHigh.get(word, 0) + 1

"""
analyze goes into every dimension and makes combinations of our dictionaries,
performs a probability analysis, calculates the ratio for each word in the combinations,
and writes the result in a text file
These names were chosen as mnemonics based on preliminary listening to extreme timepoints plus
reference to the loadings.  They are not precise, nor even necessarily correct.
"""
def analyze(dimensions):
    names = ["listening", "in-turn",  "yielding", "taking",  "bc", "cont", 
             "overlap", "pause", "particle-assisted-take", "particle-assisted-yield", 
             "short-low", "short-high", "short-louder", "short-quieter", 
             "interleaved-two", "interleaved-three","high-emphasis", "low-emphasis", 
             "joint-rise", "joint-fall"]
    for dimNum in range(1, len(dimensions)):  
        """
        Merging the dictionaries
        If it's Dim 4,7, or 10 the low hashes and high hashes are grouped together. Otherwise high is grouped with low. 
        """
        if dimNum in [4,7,10]:
            combinations = [merge(dimensions[dimNum].leftHigh, dimensions[dimNum].rightHigh), merge(dimensions[dimNum].leftLow, dimensions[dimNum].rightLow)]
        else:
            combinations = [merge(dimensions[dimNum].leftHigh, dimensions[dimNum].rightLow), merge(dimensions[dimNum].leftLow, dimensions[dimNum].rightHigh)]
            
        
        #Getting totals to use for calculations
        combinationTotals = [sum(combinations[i].values()) for i in range(2)]
        totalWords = sum(dimensions[dimNum].allWords.values())
            
        for combNum in range(2):
            dimSideName = names[((dimNum-1) * 2) + combNum]
            countFileName = "countsDim%02d%s.txt" % (dimNum, dimSideName) 
            resultFile = io.open(countFileName, "w", encoding = "utf-8")
            resultFile.write(dimensions[dimNum].filename[:-4] + " - " +  dimSideName  + ":\n")
            try:
                while True:
                    item = combinations[combNum].popitem()
                    word = item[0]
                    localOccurences = item[1]
                    globalOccurences = dimensions[dimNum].allWords.get(word, 0)
                    """
                    Contingency table: 
                                                In Dictionary                                 In rest of File
                    word occurences         localOccurences                                  globalOccurences- occureces
                    non-word occurences combinationTotals[j] - localOccurences      (totalWords - all other entries in table)
                    """
                    contingencyTable =[[localOccurences, globalOccurences - localOccurences], [combinationTotals[combNum] - localOccurences, totalWords]]
                    contingencyTable[1][1] -= (contingencyTable[0][0] + contingencyTable[0][1] + contingencyTable[1][0])
                    """
                    Debugger to confirm contingencyTable values
                    if i ==1 and combNum == 0:
                        print("Word\n", word)
                        print("Local occurences: \n", localOccurences)
                        print("Global occurences\n", globalOccurences)
                        print("All words in combination\n", combinationTotals[combNum])
                        print("All words in general\n", totalWords)
                        print(contingencyTable)
                        
                    """
                    
                    #if our sample is large enough, do chi square
                    if get_lowest_expected(contingencyTable) > 5:
                        chi2, prob, dof, expected = chi2_contingency(contingencyTable)
                    #otherwise do fishers exact test
                    else:
                        oddsratio, prob = fisher_exact(contingencyTable)
                        if prob >= 0.002:  # tons of low frequency words, so a large Bonferroni correction
                            continue
                        
                    ratio = (localOccurences/combinationTotals[combNum])/ (globalOccurences/totalWords)
                    
                    if prob < 0.02:
                        #Print the word, p value and ratio of local occurences to occurences in whole file
                        line = "Word: %5s \t p-value: %.4f,  ratio: %.2f\n" % (word, prob, ratio)
                        resultFile.write(line)
            
            #Once we run out, popping an element throws an error
            except: 
                pass

        resultFile.write("\n")
        resultFile.close()
           
                    
                    
                   
"""
search iterates through our array of searchees, looking for the bounds. 
If it's found, it returns a version of the searchee list without the element
and all the elements that come before it
Ex: searchee = [1.5, 2.5, 3.5, 4.5, 5.5] bounds = [3,4]
Result: searchee returned as [4.5, 5.5] and True because it was found
"""      
       
def search(bounds, searchee):
    for i in range(len(searchee)):
        if searchee[i] < bounds[0]:
            continue
            #Debugger
            #print("too low", searchee[i],"  ",bounds[0])
        elif searchee[i] < bounds[1]:
            return searchee[i+1:], True
        else:
            #print("too high", searchee[i]," ", bounds[0]," ", bounds[1])
            return searchee, False
    return searchee, False           
        
"""
The result of read_transcript is an array with the even indices having the words
and the odd indices having the corresponding timestamp
Ex: A[0]= "A*: Whatever the speaker said"
    A[1] = 123456_125400
"""
def read_transcript(filename):
    with io.open(filename, "r", encoding = "utf-8") as f:
        #Skipping first 9 lines of transcript that we do not need
        for unneeded in range(9):
            f.readline();
            
        fileString = f.read()
        
        #Removing all invalid characters
        fileString = fileString.replace("\n", " ").replace("\t", " ") 
        fileString = fileString.replace("⇗","").replace("→"," ")
        fileString = fileString.replace("⇘"," ").replace("一"," ")
        fileString = fileString.replace("[", "").replace("]", "")
        #                                    "                     "
        fileString = fileString.replace(chr(8220),"").replace(chr(8221), "") 
        fileString = fileString.replace("!"," ")
        # splits by  which separates every time stamp from the phrases
        return fileString.split("")





#find_thingsToLookFor finds the matching object to the filename or 
#makes a new one if not found
def find_thingsToLookFor(thingsToLookForList, name):
    for thingsToLookFor in thingsToLookForList:
        if thingsToLookFor.filename == name:
            return thingsToLookFor
        
    #name not found, make a new Transcript and return it
    thingsToLookForList.append(ThingsToLookFor(name))
    return thingsToLookForList[-1]

#merge gets all the elements in dictionary B and adds them to dictionary A
def merge(dictA, dictB):
    try:
        #pop all items and add them to dictionary A
        while True:
            item = dictB.popitem()
            word = item[0]
            occurences = item[1]
            dictA[word] = dictA.get(word, 0) + occurences
    #catch the exception when there are no more items
    except:
        pass
    return dictA


#used to get the lowest value expected to see if it is viable to use chi_square
def get_lowest_expected(A):
    #getting the sums of each row and column and storing them
    rows = [A[0][0] + A[0][1], A[1][0] + A[1][1]] 
    cols = [A[0][0] + A[1][0], A[0][1] + A[1][1]]
    
    minimum = math.inf
    
    #for every combination of rows and columns
    for row in rows:
        for col in cols:
            product = row * col
            if product < minimum:
                minimum = product
                
    #getting the total so that we can divide to get the expected value
    total = sum(rows)
    return minimum/total
    



#Main

print("Step one: Reading dimension data")
dimensions, thingsToLookForList = read_dims()
print("Step two: Reading the transcripts and inserting words into the dimensions")
count_word_occurences(dimensions, thingsToLookForList)
print("Step three: Compute ratios and p-values and print the results to the results file")
analyze(dimensions)
print("All done")






"""        
Debugger for general use

print(dimensions)
for thingsToLookFor in thingsToLookForList:
    print(thingsToLookFor.filename)
    for timestamp in thingsToLookFor.timestampInstances:
        print(timestamp.dimInd)
        print(timestamp.points)
        
Two functions below to handle printing of every dictionary created.
call traverse_dimensions(dimensions) to utilize it     

#Passes every dictionary in the list of dimensions to the writing function
def traverse_dimensions(dimensions):
    for dimension in dimensions[1:]:
        #adding offset to take off ".txt"
        offset = -4
        
        write(dimension.leftLow, dimension.filename[:offset] + "_LL_Results.txt")
        write(dimension.leftHigh, dimension.filename[:offset] + "_LH_Results.txt")
        write(dimension.rightLow, dimension.filename[:offset] + "_RL_Results.txt")
        write(dimension.rightHigh, dimension.filename[:offset] + "_RH_Results.txt")
        write(dimension.allWords, dimension.filename[:offset] + "_all_words.txt")
        
#Sorts hash by occurences and prints to a file
def write(dictionary, filename):
    output = io.open(filename,"w",encoding ="utf-8")
    #for every instance in our dictionary sorted descending by occurences
    for keyVal in sorted(dictionary.items(), key = lambda dictionary:(dictionary[1], dictionary[0]), reverse = True):
        word = keyVal[0]
        occurences = keyVal[1]
        
        #layout the format of our output and write it into the file
        printString = "Word: {}\tOccurences: {}\n"
        output.write(printString.format(word, occurences))
    output.close()
"""
